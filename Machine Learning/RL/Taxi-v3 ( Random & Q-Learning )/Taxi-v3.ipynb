{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SxG7wu_wZSVJ"
      },
      "outputs": [],
      "source": [
        "# Step 0: Install and import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzY6U32VaGgx",
        "outputId": "68044a00-84e0-4b08-89d7-41181afbeffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JlZlv0FbaNyw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gD-MgfAlaN1A"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uQ4AvezJaN3Y"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wRJyqAOIaN5o"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create the Q-table and initialize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-RWO32xaN73",
        "outputId": "e51014d0-a05f-48ba-f191-021cbd809918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  500  possible states\n",
            "There are  6  possible actions\n"
          ]
        }
      ],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\") # up, down, right, down, take-on, take-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcQ16UTaaN-A",
        "outputId": "baab8571-c3af-494f-c0a2-9794e070750a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "(500, 6)\n"
          ]
        }
      ],
      "source": [
        "# Create our Q table with state_size rows and action_size columns (500x6)\n",
        "Q = np.zeros((state_space, action_space))\n",
        "print(Q)\n",
        "print(Q.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "25-6PbEnaGtg"
      },
      "outputs": [],
      "source": [
        "# Step 3: Define the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xBpfHupHaGq5"
      },
      "outputs": [],
      "source": [
        "total_episodes = 25000        # Total number of training episodes (The more, the better)\n",
        "total_test_episodes = 10      # Total number of test episodes (just for show to us)\n",
        "max_steps = 200               # Max steps per episode (after 200 step the episode close)\n",
        "\n",
        "learning_rate = 0.01          # Learning rate\n",
        "gamma = 0.99                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.001           # Minimum exploration probability\n",
        "decay_rate = 0.01             # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ELX2LYYlcyv4"
      },
      "outputs": [],
      "source": [
        "# Step 4: Define the epsilon-greedy policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2P5aaCbQaGv5"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Q, state): # Q = score of now  state = what state do we be\n",
        "  # if random number > greater than epsilon --> exploitation (greedy)\n",
        "  if(random.uniform(0,1) > epsilon):\n",
        "    action = np.argmax(Q[state]) # maximum action\n",
        "  # else --> exploration (random)\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "J_3JLYx2cyyJ"
      },
      "outputs": [],
      "source": [
        "# Step 5: Define the Q-Learning algorithm and train our agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VJznTA6Xcy0Q"
      },
      "outputs": [],
      "source": [
        "for episode in range(total_episodes): # in each episode ...\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "\n",
        "    for step in range(max_steps): # 200 step is max\n",
        "        #\n",
        "        action = epsilon_greedy_policy(Q, state) # select action\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action) # it is a options of this library and we can create by our selves\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]     Update and Learning\n",
        "        Q[state][action] = Q[state][action] + learning_rate * (reward + gamma * np.max(Q[new_state]) - Q[state][action])\n",
        "        # If done : finish episode\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "        # Our new state is state\n",
        "        state = new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PWMLJh1Mcy2I"
      },
      "outputs": [],
      "source": [
        "# Step 6: Let's watch our autonomous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ih0jIF8cy4R",
        "outputId": "1f5aab17-bf54-44b2-982e-1f3090f82920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "****************************************************\n",
            "EPISODE  5\n",
            "****************************************************\n",
            "EPISODE  6\n",
            "****************************************************\n",
            "EPISODE  7\n",
            "****************************************************\n",
            "EPISODE  8\n",
            "****************************************************\n",
            "EPISODE  9\n",
            "Score over time: 8.0\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "rewards = []\n",
        "\n",
        "frames = []\n",
        "for episode in range(total_test_episodes): # create episode for show to us\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "    for step in range(max_steps):\n",
        "        env.render() # show enviroment\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(Q[state][:]) # we don't select action with e-greedy because we learning before and we take action just from Q-Table\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        total_rewards += reward\n",
        "\n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            #print (\"Score\", total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJjgdhEScy6Y",
        "outputId": "c2a81fcf-b68b-4efd-81a8-d0b91cb18b79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ],\n",
              "       [-2.98146739, -3.04552473, -3.34395812, -3.04631192,  9.6220697 ,\n",
              "        -3.33142803],\n",
              "       [-1.45027051, -1.29697924, -1.48309371, -1.15478664, 14.11880599,\n",
              "        -1.32393218],\n",
              "       ...,\n",
              "       [-0.72675663, -0.61750394, -0.74625407, -0.72568389, -0.77401061,\n",
              "        -0.97859423],\n",
              "       [-2.23155149, -2.22898763, -2.25532573, -1.45321135, -2.67444219,\n",
              "        -2.6730212 ],\n",
              "       [-0.05084896, -0.0260451 , -0.0199    ,  2.40152549, -0.28749926,\n",
              "        -0.46353213]])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
